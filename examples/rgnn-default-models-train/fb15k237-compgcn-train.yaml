# Default settings for CompGCN on FB15k-237
import: [rgnn_encoder, lookup_embedder]

job.type: train
dataset.name: fb15k-237
model: compgcn

compgcn:
  entity_embedder:
    type: lookup_embedder
    initialize: xavier_normal_
    regularize: ''
    dim: 100
    +++: +++
  relation_embedder:
    type: lookup_embedder
    initialize: xavier_normal_
    regularize: ''
    dim: 100
    +++: +++
  encoder:
    type: rgnn_encoder
    # CompGCN specific RGNN parameters:
    layer_type: message_passing 
    message_passing_args:
      edge_norm: True
      emb_propagation_dropout: 0.3  
      propagation: direction # use a different weight matrix for in-direction,
                              # out-direction and the self-loop
      composition: ccorr  # sub, mult and ccorr in original implementation
      message_weight: False
      attention: False
    activation: tanh
    rel_transformation: linear
    num_layers: 1
    emb_entity_dropout: 0.1  # used on the entity output of the rgnn layer
    self_edge_dropout: 0.0
    edge_dropout: 0.0
    bias: False 
    weight_decomposition: None # relation_basis 
    num_blocks_or_bases: -1
  decoder: # specify which scorer to use as a decoder in the RGNN, one of the
           # scoring functions. The original CompGCN model supports TransE,
           # ConvE and Distmult but in principle any other function can be imported
           # or passed
    model: reciprocal_relations_model
    type: reciprocal_relations_model
    base_model.type: conve
      #type: kge_model
    reciprocal_relations_model.base_model.type: conve
    base_model:
      round_dim: True
      entity_embedder: # see as output size of Rgnn
        dim: 200            
        +++: +++
      relation_embedder:
        dim: 200            
        +++: +++
    scorer: ConvEScorer 

train:
  type: KvsAll
  loss: bce
  max_epochs: 500
  batch_size: 128
  optimizer.default:
    type: Adam
    args:
      amsgrad: False
      eps: 1e-08
      lr: 0.001
      weight_decay: 0.0

KvsAll:
  label_smoothing: 0.1

random_seed:
  numpy: 41504
  torch: 41504
